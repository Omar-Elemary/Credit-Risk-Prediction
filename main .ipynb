{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65a13fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except:\n",
        "    LIGHTGBM_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4369e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55437606",
      "metadata": {},
      "source": [
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\\nStatistical Summary:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5c62c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric columns: {numeric_cols}\")\n",
        "print(f\"\\nCategorical columns: {categorical_cols}\")\n",
        "\n",
        "if 'loan_status' in df.columns:\n",
        "    target_col = 'loan_status'\n",
        "elif 'default' in df.columns:\n",
        "    target_col = 'default'\n",
        "elif 'target' in df.columns:\n",
        "    target_col = 'target'\n",
        "else:\n",
        "    target_col = df.columns[-1]\n",
        "\n",
        "print(f\"\\nTarget column: {target_col}\")\n",
        "print(f\"\\nTarget distribution:\\n{df[target_col].value_counts()}\")\n",
        "print(f\"\\nTarget distribution (%):\\n{df[target_col].value_counts(normalize=True) * 100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b9a076",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "df[target_col].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Class', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d56c34f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(numeric_cols) > 0:\n",
        "    n_cols = min(3, len(numeric_cols))\n",
        "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols):\n",
        "        if idx < len(axes):\n",
        "            axes[idx].hist(df[col].dropna(), bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "            axes[idx].set_title(f'Distribution of {col}', fontsize=11, fontweight='bold')\n",
        "            axes[idx].set_xlabel(col, fontsize=10)\n",
        "            axes[idx].set_ylabel('Frequency', fontsize=10)\n",
        "            axes[idx].grid(alpha=0.3)\n",
        "    \n",
        "    for idx in range(len(numeric_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Distribution of Numeric Features', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e20e9d",
      "metadata": {},
      "source": [
        "if len(numeric_cols) > 1:\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix of Numeric Features', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa7520a",
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(categorical_cols) > 0:\n",
        "    n_cols = min(3, len(categorical_cols))\n",
        "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(categorical_cols):\n",
        "        if idx < len(axes):\n",
        "            value_counts = df[col].value_counts()\n",
        "            axes[idx].bar(range(len(value_counts)), value_counts.values, color='coral', edgecolor='black', alpha=0.7)\n",
        "            axes[idx].set_title(f'Distribution of {col}', fontsize=11, fontweight='bold')\n",
        "            axes[idx].set_xlabel(col, fontsize=10)\n",
        "            axes[idx].set_ylabel('Count', fontsize=10)\n",
        "            axes[idx].set_xticks(range(len(value_counts)))\n",
        "            axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    for idx in range(len(categorical_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle('Distribution of Categorical Features', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6346d92e",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_processed = df.copy()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
        "\n",
        "print(f\"Missing values after handling: {df_processed.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbf7fa7",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    if col != target_col:\n",
        "        le = LabelEncoder()\n",
        "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "if df_processed[target_col].dtype == 'object':\n",
        "    le_target = LabelEncoder()\n",
        "    df_processed[target_col] = le_target.fit_transform(df_processed[target_col])\n",
        "\n",
        "print(\"Categorical encoding completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f2013c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_processed.drop(columns=[target_col])\n",
        "y = df_processed[target_col]\n",
        "\n",
        "for col in X.select_dtypes(include=[np.number]).columns:\n",
        "    Q1 = X[col].quantile(0.25)\n",
        "    Q3 = X[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    X[col] = np.clip(X[col], lower_bound, upper_bound)\n",
        "\n",
        "print(\"Outlier treatment completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2401bf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "if 'loan_amnt' in numeric_features:\n",
        "    X['loan_amnt_log'] = np.log1p(X['loan_amnt'])\n",
        "if 'annual_inc' in numeric_features:\n",
        "    X['annual_inc_log'] = np.log1p(X['annual_inc'])\n",
        "if 'loan_amnt' in numeric_features and 'annual_inc' in numeric_features:\n",
        "    X['debt_to_income'] = X['loan_amnt'] / (X['annual_inc'] + 1)\n",
        "if 'loan_amnt' in numeric_features and 'int_rate' in numeric_features:\n",
        "    X['total_interest'] = X['loan_amnt'] * X['int_rate'] / 100\n",
        "\n",
        "for col in numeric_features:\n",
        "    if X[col].std() > 0:\n",
        "        X[f'{col}_squared'] = X[col] ** 2\n",
        "        X[f'{col}_sqrt'] = np.sqrt(np.abs(X[col]))\n",
        "\n",
        "print(f\"Feature engineering completed. New shape: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df80490",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf88711",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'SVM': SVC(probability=True, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    models['XGBoost'] = xgb.XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1)\n",
        "\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    models['LightGBM'] = LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
        "\n",
        "print(f\"Initialized {len(models)} models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c7c6567",
      "metadata": {},
      "source": [
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name in ['SVM', 'KNN']:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    except:\n",
        "        roc_auc = 0.0\n",
        "    \n",
        "    results[name] = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'ROC-AUC': roc_auc\n",
        "    }\n",
        "    \n",
        "    trained_models[name] = model\n",
        "    print(f\"{name:25s} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nModel training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068616ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['Accuracy'] for m in results.keys()],\n",
        "    'Precision': [results[m]['Precision'] for m in results.keys()],\n",
        "    'Recall': [results[m]['Recall'] for m in results.keys()],\n",
        "    'F1-Score': [results[m]['F1-Score'] for m in results.keys()],\n",
        "    'ROC-AUC': [results[m]['ROC-AUC'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "comparison_df = comparison_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62dc59f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_differences = {}\n",
        "base_accuracy = comparison_df.iloc[0]['Accuracy']\n",
        "base_model = comparison_df.iloc[0]['Model']\n",
        "\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    diff = base_accuracy - row['Accuracy']\n",
        "    accuracy_differences[row['Model']] = diff\n",
        "    print(f\"{row['Model']:25s} - Accuracy: {row['Accuracy']:.4f} | Difference from {base_model}: {diff:.4f}\")\n",
        "\n",
        "print(f\"\\nBest Model: {base_model} with Accuracy: {base_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e903b4",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(comparison_df)))\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    bars = ax.barh(comparison_df['Model'], comparison_df[metric], color=colors)\n",
        "    ax.set_xlabel(metric, fontsize=12)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim(0, 1)\n",
        "    \n",
        "    for i, (bar, val) in enumerate(zip(bars, comparison_df[metric])):\n",
        "        ax.text(val + 0.01, i, f'{val:.4f}', va='center', fontsize=9)\n",
        "    \n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Model Performance Metrics Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9d705f",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "bars = plt.barh(comparison_df['Model'], comparison_df['Accuracy'], color=colors)\n",
        "plt.xlabel('Accuracy Score', fontsize=12)\n",
        "plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xlim(0, 1)\n",
        "\n",
        "for bar, val in zip(bars, comparison_df['Accuracy']):\n",
        "    plt.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "             f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a30d4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "metrics_data = comparison_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
        "sns.heatmap(metrics_data, annot=True, fmt='.4f', cmap='YlOrRd', \n",
        "            cbar_kws={'label': 'Score'}, linewidths=0.5)\n",
        "plt.title('Model Performance Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.ylabel('Model', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec39b0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for name in results.keys():\n",
        "    if name in ['SVM', 'KNN']:\n",
        "        y_pred_proba = trained_models[name].predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = trained_models[name].predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = results[name]['ROC-AUC']\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves - All Models Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a88d9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_models = len(results)\n",
        "n_cols = 3\n",
        "n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
        "axes = axes.flatten() if n_models > 1 else [axes]\n",
        "\n",
        "for idx, (name, result) in enumerate(results.items()):\n",
        "    if name in ['SVM', 'KNN']:\n",
        "        y_pred = trained_models[name].predict(X_test_scaled)\n",
        "    else:\n",
        "        y_pred = trained_models[name].predict(X_test)\n",
        "    \n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                cbar_kws={'label': 'Count'}, linewidths=0.5)\n",
        "    axes[idx].set_title(f'{name}\\nAccuracy: {result[\"Accuracy\"]:.4f}', \n",
        "                        fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
        "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
        "\n",
        "for idx in range(n_models, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c84157",
      "metadata": {},
      "source": [
        "cv_results = {}\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    if name in ['SVM', 'KNN']:\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
        "                                    cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    else:\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, \n",
        "                                    cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    \n",
        "    cv_results[name] = {\n",
        "        'Mean CV Score': cv_scores.mean(),\n",
        "        'Std CV Score': cv_scores.std()\n",
        "    }\n",
        "    print(f\"{name:25s} - CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
        "\n",
        "cv_df = pd.DataFrame({\n",
        "    'Model': list(cv_results.keys()),\n",
        "    'Mean CV Accuracy': [cv_results[m]['Mean CV Score'] for m in cv_results.keys()],\n",
        "    'Std CV Accuracy': [cv_results[m]['Std CV Score'] for m in cv_results.keys()]\n",
        "}).sort_values('Mean CV Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(cv_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93783bfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_comparison = comparison_df.merge(cv_df, on='Model')\n",
        "final_comparison['Overall Score'] = (\n",
        "    final_comparison['Accuracy'] * 0.25 +\n",
        "    final_comparison['Precision'] * 0.20 +\n",
        "    final_comparison['Recall'] * 0.20 +\n",
        "    final_comparison['F1-Score'] * 0.20 +\n",
        "    final_comparison['ROC-AUC'] * 0.10 +\n",
        "    final_comparison['Mean CV Accuracy'] * 0.05\n",
        ")\n",
        "final_comparison = final_comparison.sort_values('Overall Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"FINAL MODEL RANKING\")\n",
        "print(\"=\"*90)\n",
        "print(final_comparison[['Model', 'Accuracy', 'Precision', 'Recall', \n",
        "                        'F1-Score', 'ROC-AUC', 'Mean CV Accuracy', 'Overall Score']].to_string(index=False))\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\nðŸ† Best Model: {final_comparison.iloc[0]['Model']}\")\n",
        "print(f\"   Accuracy: {final_comparison.iloc[0]['Accuracy']:.4f}\")\n",
        "print(f\"   F1-Score: {final_comparison.iloc[0]['F1-Score']:.4f}\")\n",
        "print(f\"   ROC-AUC: {final_comparison.iloc[0]['ROC-AUC']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c36c22cf",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "bars = plt.barh(final_comparison['Model'], final_comparison['Overall Score'], \n",
        "               color=plt.cm.viridis(np.linspace(0, 1, len(final_comparison))))\n",
        "\n",
        "plt.xlabel('Overall Score', fontsize=12)\n",
        "plt.title('Model Ranking by Overall Performance Score', fontsize=14, fontweight='bold')\n",
        "plt.xlim(0, 1)\n",
        "\n",
        "for bar, score in zip(bars, final_comparison['Overall Score']):\n",
        "    plt.text(score + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "            f'{score:.4f}', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc6e6d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_diffs = []\n",
        "model_names = []\n",
        "\n",
        "for idx in range(len(comparison_df)):\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    diff = comparison_df.iloc[0]['Accuracy'] - comparison_df.iloc[idx]['Accuracy']\n",
        "    accuracy_diffs.append(diff)\n",
        "    model_names.append(comparison_df.iloc[idx]['Model'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.barh(model_names, accuracy_diffs, color='crimson', alpha=0.7)\n",
        "plt.xlabel('Accuracy Difference from Best Model', fontsize=12)\n",
        "plt.title('Accuracy Differences from Best Model', fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "for bar, val in zip(bars, accuracy_diffs):\n",
        "    plt.text(val + 0.001 if val >= 0 else val - 0.001, \n",
        "             bar.get_y() + bar.get_height()/2, \n",
        "             f'{val:.4f}', va='center', fontsize=9, \n",
        "             ha='left' if val >= 0 else 'right')\n",
        "\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAccuracy differences from {comparison_df.iloc[0]['Model']} (Best Model):\")\n",
        "for name, diff in zip(model_names, accuracy_diffs):\n",
        "    print(f\"  {name:25s}: {diff:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b443607",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02030231",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ed24cb43",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e946da",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c24d96",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "88ade92f",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4d541b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6697321",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1cbd0b91",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
